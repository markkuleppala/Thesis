\chapter{Virtualization and Cloud Computing}
\label{chapter:cloudcomputing}

Virtualization and cloud computing has enabled a wave of new generation software to be served without local servers. These services are ubiquitous and highly available, served from over the internet. A logical step forward from centralized cloud computing is to bring the services closer to the end-user with Far Edge Cloud. This deployment model allows serving localized and high-performing applications with low-latency connections via mobile or landline connections. However, these new and complex deployment models are missing robust security features to guardrail the user and data confidentiality which needs more attention.

This chapter introduces different virtualization models, including containers, the latest addition, and the foundation for cloud computing. Far Edge Cloud is introduced as well as the new applications this distributed cloud computing deployment enables. In the last sections, the security concerns are demonstrated with the possible solutions to overcome these challenges.

\section{Virtualization}

Virtualization is a technology that allows the creation of valuable IT services using resources that are traditionally bound to hardware. The term virtualization refers to the abstraction of computing resources from applications and end-users consuming the service \cite{Xing2012}. Virtualization allows hardware operators to use the physical machine's full capacity by simultaneously distributing the computing power to multiple users or environments. \cite{RedHat} In the context of telco, virtualization is the main objective to replace the costly dedicated hardware by implementing several centralized control plane functions and other services with distributed solutions. These services and solutions may be allocated on-demand over a pool of dependable, dynamically contracted computing and networking resources that are easy to manage\cite{Bosch2011}. Virtualization brings down the expenses arising from acquiring, updating, and managing the hardware \cite{Lingayat2018}\cite{Toimela2017}.

Virtualization involves the construction of an isomorphism that maps a virtual guest system to a real host. This isomorphism maps the guest state to the host state \cite{Xing2012}. Virtualization abstracts the underlying resources such as CPU, GPU, storage, network, and memory from the application running on the host. Previously the virtualization support was purely software-based, which included latency for the application requiring virtualization. The introduction of x86 virtualization for CPUs by Intel's VT-d and AMD's AMD-V allowed minimal software to be used with virtualization, thus reducing the overhead for CPU operations with direct access.

\section{Virtualization types}

A traditional physical server has applications running directly on the host OS. The solution limits overhead from extra layers such as a hypervisor or VM but compromises the usage of resource efficiency, security and limits compatibility. Two virtualization technologies, hypervisor-based virtualization and container-based virtualization, have been introduced to improve these features. Hypervisor-based virtualization provides strong isolation of a complete operating system, whereas container-based virtualization strives to isolate processes from other processes at little resource costs \cite{Eder2016}. Figure \ref{fig:VirtualizationTypes} describes the stack of these two virtualization types and a traditional physical server.

Hypervisor-based virtualization involves hypervisors to emulate hardware for creating virtual machines and wraps each application instance running inside a VM with a dedicated guest OS, binaries, and libraries. The hypervisor allows emulating another computer fully. Therefore, it is possible to emulate other types of devices, for example, a smartphone or a gaming console \cite{Eder2016}. This emulation is helpful, especially when developing software for mobile platforms allowing testing without the need of a target device. The inclusion of an additional layer of hypervisor over the host operating system is highly secure and compatible with various host operating systems and processor architectures via hardware emulation \cite{Lingayat2018}. However, the added layers depreciate the virtual machine's performance significantly as the hypervisor directs traffic between physical and virtual hardware. Resources of the server are wasted, primarily when the same server hosts multiple VMs.

Recent evolution is a lightweight alternative to the virtual machine called containers. Container-based virtualization got gained popularity when Docker, a tool to create, manage, distribute, and run containers, got much attention by combining different technologies to a powerful virtualization software \cite{Eder2016}. The advent of container management and orchestration systems Docker and Kubernetes have profoundly changed the ecosystem of building, shipping, and deploying multi-tier distributed applications in the cloud \cite{Gao2017}. Container-based virtualization has proven very efficient regarding performance and orchestration, and many industries have migrated their virtualized environment to run on Linux containers. Containers share parts of the host operating systems kernel and the host's libraries and binaries where appropriate and isolate each container by encapsulating them with their required services and packages. These containers can also include application-specific binaries and libraries. Linux containers utilize less storage space and consume optimal computational power, giving an improvement in performance in resource overhead, space requirements, and speed \cite{Toimela2017}\cite{Lingayat2018}. Containers are highly portable and compatible due to the standardized packaging format managed by Open Container Initiative (OCI)\cite{OCI}. Managing even thousands of containers can be easily achieved with container orchestration and scheduling tools such as Kubernetes. Containers are isolated from each other by using various packaging techniques such as Control Groups and Linux kernel namespaces and utilize kernel features to create an isolated environment for processes \cite{Flauzac2020}.

In contrast to hypervisor-based virtualization, containers do not get their own virtualized hardware but use the host system's hardware. Not having to boot a complete operating system and emulate hardware enables containers to start in a few milliseconds and be more efficient than classical virtual machines \cite{Eder2016}. The lack of security features comes with the cost of frail security. Thus Chapter \ref{section:security} points out that the security mechanisms of Linux containers are partial against more advanced attacks. Chapter \ref{chapter:katacontainers} introduces Kata Containers to improve the security of containerized environments. This recently developed container runtime solution combines the security of virtual machines and the speed of containers with the help of custom container runtime.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=13.5cm]{images/VirtualizationTypes.pdf}
    \caption{Physical server and two types of virtualization}
    \label{fig:VirtualizationTypes}
  \end{center}
\end{figure}

\section{Cloud computing}

With the help of hardware virtualization, data centers offer on-demand availability of computer resources such as computing power or data storage. This pay-per-use offering is described as cloud computing. Cloud computing offers customers the ability to start businesses without having to pay significant upfront capital expenses and the exhibition to scale the IT infrastructure up and down as a business evolves without worrying about over or under-provisioning \cite{Xing2012}. It has been a vital enabler in Information Technologies, allowing the success of cloud applications such as Google Docs, Slack, and Dropbox. The ubiquitous and connected computing allows higher usage of resources and access from anywhere via the internet, thus improving performance, reducing costs, and allowing easy access to hosted applications. Public cloud services end-user spending worldwide has been growing steadily and is expected to grow 40\% from 219 billion euro in 2020 to 307 billion euro only in two years \cite{PublicCloudStatista}.

The general idea behind cloud computing technology dates back to the emergence of grid computing in the early 1990s to make computing power as easy to access as an electric power grid. This cloud model promotes the cloud as a pool of highly available virtualized resources across the internet that follows a pay-per-use model and can be dynamically reconfigured to satisfy the user requests by provisioning virtual machines. Cloud computing is a service model for IT provisioning, often based on virtualization and distributed computing technologies \cite{Lombardi2011}. \cite{Xing2012}

Generally, the cloud is hosted on multiple geographically distributed but centralized servers. Cloud computing offers four different service models: public, private, hybrid, and community clouds. Public cloud platforms, such as Microsoft Azure or Red Hat OpenShift, provide users with scalable resources, high availability, and fast connections. These platforms are open to all users who are willing to pay for the services. In contrast, the private cloud serves only selected and authenticated customers. Private clouds are usually deployed on-premises within a corporate firewall by businesses with additional control and privacy requirements over the network. A hybrid cloud is a combination of both, made up of on-premises infrastructure, private cloud services, and a public cloud. The hybrid approach brings enterprise with agility to use the best suitable solution for each occasion\cite{NetApp}. A community cloud is a resource pool that consists of several providers, which a particular group of users can share. Although different cloud computing deployment models cater to different consumers, they all have common characteristics such as massive scale, resilient computing, homogeneity, virtualization, low-cost software, geographic distribution, service orientation, and advanced security technologies. \cite{MicrosoftAzure}\cite{Taleb2017}\cite{Xing2012}

\section{Multi-access Edge Computing}

By 2020, 73\% of world population had mobile subscription \cite{StatistaMobileSubscribers}. Together with the Internet of Things (IoT) devices equipped with sensors, the data produced is expected to grow 53\% annually \cite{Ning2019}. The constant data production and stream adds significant stress to the mobile network. The huge volume of data in a cloud-based infrastructure has caused unbearable transmission delays and a low-quality user experience.

MEC is an emergent architecture that extends cloud computing services to the edge of networks leveraging mobile base Radio Access Network (RAN). RAN improves the delivery of content and applications for end-user. As a promising edge technology, it can be applied to mobile, wireless, and wireline scenarios using software and hardware platforms located at the network edge in the proximity of end-users. MEC seamlessly integrates several application service providers and vendors toward enterprises, mobile subscribers, and other vertical segments. It is an essential component in the 5G architecture supporting various innovative applications and services where ultra-low latency is required. \cite{Abbas2018}

Figure \ref{fig:AirFrame} describes the differences in edge and central data centers.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=13.5cm]{images/AirFrame.png}
    \caption{Far edge comparison to central data center \cite{AirFrameOpenEdgeServer}}
    \label{fig:AirFrame}
  \end{center}
\end{figure}

MEC improves network efficiency by processing and storing data on the mobile network cell instead of hauling it entirely back to regional or central data centers for processing. The MEC processes the data partially, and backhauls pre-processed data to the data center for more advanced processing or processes the data entirely in the node and responds to the user. For example, large public venues, such as stadiums or arenas, are good candidates for MEC, especially where localized venue services are important. In this use case, a video created at a sports event or concert is served to on-site consumers from a MEC server, running appropriate applications located on the stadium premises. This video traffic is locally stored, processed, and delivered directly to users at the event. Thus, not requiring to backhaul to a centralized core network entirely for processing and then to be returned to the user at the venue. Some data requires extensive processing, which is achieved with the help of a central server. For example, a surveillance video feed of traffic on the road is pre-processed in the MEC to optimize packaging and bitrate and transmitted to the central server for training machine learning models to detect possible collisions or dangerous encounters. The local processing of data reduces perceived latency on end-user and limits stress on the backhaul network. \cite{Brown2016}

MEC covers two edge entities: aggregated edge and far edge. Aggregated edge usually locates at most 200 to 350 kilometers away from the end-user and guarantees 4 to 10 millisecond round trip time (RTT). The RTT is low compared to central data center latency. However, the latency of MEC is not low enough for mission-critical and real-time applications requiring less than 1 ms latency. To meet the requirements of real-time and mission-critical applications, for example, self-driving cars and factory robots that require ultra-low latency reply from the applications to respond to a fast-changing environment, MEC needs to extend cloud computing capabilities closer to the edge and terminal devices \cite{Ning2019}.

\subsection{Far Edge Cloud}

Far edge cloud locates at most 20 to 40 kilometers away from the end-user and can offer sub-one millisecond RTT, as defined in service level agreement of the 5G network \cite{Parvez2018}. The ultra-low latency is critical for applications requiring an instant reply from the server. Offloading tasks, computing, and storage to the nearby cloud allow thin clients, such as cellphones or dashboards, with limited resources to support applications with resource-heavy requirements, such as applying machine learning models to the obtained video feed in real-time. Migration of tasks to resource-rich centralized servers also helps power agnostic user equipment to save battery life. Far edge cloud setup is minimal in size, footprint, and power budget. It is flexible to install as it can locate in an office or apartment building with no requirement for extensive server chassis. \cite{AirFrameOpenEdgeServer}

Far edge cloud units are highly localized. A network might consist of hundreds or thousands of these units. The data processed does not always travel further away from the end-user, enhancing the security and integrity of the data. The localized data schemes also help service providers, as they are not obliged to apply multiple data protection laws, GDPR, for example, at the same time. FEC units also handle sensitive, user-related data. Thus, it is crucial to secure the infrastructure from external threats and internal attacks, such as third-party applications hosted on the infrastructure.

\subsection{Applications}
\label{subs:applications}

MEC architecture enables third-party applications on the platform, unlocking a new revenue stream for mobile network operators. Some recognized applications harnessing MEC include augmented reality (AR), video analytics, and connected vehicles.

AR enables a real-world user experience by combining video feed of the local environment and embedded virtual objects simultaneously. Recent AR applications have become adaptive in sound and visual components, enhancing TV programs, sports, games, and object recognition. AR applications often demand high computational resources, low latency for a reasonable quality of experience, and high bandwidth. MEC can empower AR applications by maximizing throughput by offering computational resources nearby the end-user within low-latency and high throughput channels instead of relying on the core network. \cite{Abbas2018}

Surveillance cameras traditionally have been streaming data back to the server, which decides how to perform data analysis. The growing number of these cameras adds enormous stress to the network with the constant data streams. In this example, MEC will be beneficial by implementing intelligence to the device by transmitting data only when motion is detected and compressing the stream bitrate to minimize transmission bandwidth. Also, it can help public safety with the detection of traffic jams, accidents, or forest fires.

Edge computing technology plays a principal role in website performance optimization, such as caching HTML content, reorganizing web layout, and resizing web components. HTTP requests from users pass the edge server, which helps with content delivery. The server handles user requests by performing several tasks to load web pages on the user device interface. \cite{Abbas2018}

Connected vehicles are facilitated with a cellular connection, allowing them to communicate with other users on the road. MEC-based road communication system could allow two-way communication between vehicles via roadside units. For example, one vehicle could warn another vehicle about upcoming road jams or approaching pedestrians. The roadside unit could also warn passing cars about dangerous conditions based on the sensors equipped in it. \cite{Abbas2018}

\section{Security concerns}
\label{section:security}

Integrity and confidentiality concerns are still problems in cloud computing that call for effective and efficient solutions. Cloud nodes are inherently more vulnerable to cyber-attacks and breaches than traditional physical server solutions, given their size and the underlying complexity of services bringing unprecedented exposure to third parties of services and interfaces \cite{Lombardi2011}. Container deployments are exposed to four kinds of security threats: (i) protecting a container from applications inside it, (ii) inter-container protection, (iii) protecting the host from containers, and (iv) protecting containers from a malicious or semi-honest host \cite{Flauzac2020}. In this thesis, the focus is on threats (i), (ii), and (iii).

Cloud and edge computing security cover the same threats as core computing. However, these platforms are generally not offering the same security-rich features and security policies and procedures. Edge computing is typically more vulnerable to local introspection attacks. Edge computing units might be vulnerable to Denial-of-Service attacks, which might drain the unit's resources, thus rendering all hosted services useless. Likewise, multi-tenant hosts present a unique challenge to security for a Kubernetes cluster. Underlying host resources can be consumed by another user of the system, resulting in reduced workload capacity \cite{Edwards2019}. Furthermore, malicious antagonists can misuse virtual resources and affect the whole system itself and the connected devices such as IoT sensors or user equipment. \cite{EdgeComputing5G}\cite{Abbas2018}

Cloud and edge computing allow service providers and MNOs for more flexible and cost-efficient service deployment due to shared and decentralized infrastructure. Especially MNOs seek to unlock extra revenue streams by offering their infrastructure to host applications such as mentioned in Chapter \ref{subs:applications}. In order to mitigate costs from the application, it is resource-efficient to run multiple instances or applications from various parties on a single server. This approach applies to companies that own dedicated servers or cloud computing providers with various data center options. These multi-tenant environments may host applications also from untrusted parties.

Docker containers are by default secured with multiple isolation and security features such as namespaces, Control Groups, Capabilities, Seccomp, and Apparmor. A namespace is a security feature in the Linux kernel allowing resource isolation to a group of processes. This feature enables a group of processes to use their isolated instance of system resources, invisible from another namespace. The kernel provides six namespaces to isolate different resources: IPC, Mount, PID, Network, UTS, and User. Each of these resources provides isolation to their governed resources. Control Groups (cgroups) isolate and limit hardware resources to a process or a process group. Capabilities is a kernel feature dividing superuser privileges into separate units, known as capabilities, which can be independently enabled and disabled. Seccomp offers a way to limit the number of system calls available for applications to communicate with the kernel. Apparmor enables a way to restrain processes to a limited set of resources, mainly to limit access and mount options to filesystems paths. \cite{Flauzac2020} \cite{Gao2017}

Regardless of the isolation offered by Docker with cgroups and namespaces, the containers still share the same kernel. Exploiting a Linux kernel vulnerability exposes all applications to the attacker. This issue was noticed in 2019 \cite{CVE-2020-14386}\cite{CVE-2019-5736} when attackers could elevate their access to the host root. The escaping of container and horizontal traversal via shared kernel creates a security thread of application data integrity. The virtualization technique domain is buoyant with many competing emerging technologies for hardening containers and VMs, solving the equation of isolation versus overhead. This thesis researches and evaluates Kata Containers to adding an extra layer of security to avoid the container escapes on host compromise on a shared platform with minimal overhead. \cite{EdgeComputing5G}

\section{Isolation mechanisms}
\label{section:isolation}

Sharing the kernel of the host operating system exposes applications to potential security threats. Companies handling and hosting sensitive data in cloud computing environments have developed new ways to solve the trade-off between isolation and speed. In this thesis, the focus is on surveying container runtimes. Kata Containers is a promising container runtime that unifies the security advantages of a virtual machine with the speed and flexibility of containers. In Kata Containers, each container is equipped with its lightweight virtual machine and a tiny kernel, providing container isolation via hardware virtualization, which improves the security layer \cite{Kumar2020}.

Kata Containers is not the sole option providing secure container runtime nor isolation in the container kernel layer. Microsoft offers a container instance isolation option inside Azure with Hyper-V. The hardware isolation is based on VMs, likewise in Kata Containers and Firecracker, thus leveraging the additional kernel layer. \cite{Hyper-V}

gVisor provides a second isolation method, differing from Kata Containers and Firecracker. gVisor intercepts application system calls and acts as the guest kernel without translating through virtualized hardware. The architecture can be thought of as a merged kernel and Virtual Machine Monitor (VMM). gVisor includes OCI runtime runsc, which provides the isolation boundary between the application and host kernel. Google develops gVisor, and it is harnessed in various Google's cloud products such as Kubernetes Engine \cite{GKE} and Cloud Run \cite{CloudRun}. \cite{Debab2021}\cite{gVisor}

A second approach for container isolation is IBM Nabla \cite{Nabla}. Nabla containers use library OS, also known as unikernel, to avoid system calls and reduce the attack surface. Nabla is based on a custom VMM named Nabla Tender to manage lightweight VMs executing unikernels. Nabla containers only use seven system calls, blocking all others via a Linux seccomp policy. The Nabla Tender intercepts hypercalls related to storage and network from unikernel VMs and translates them into syscalls to the host. \cite{Debab2021}

All the before-mentioned projects are open source and actively maintained. However, Nabla seems to be slightly less active in comparison to the other projects. Commercial and enterprise cloud platforms, such as Amazon Web Services (AWS), uses Firecracker, and GKE uses gVisor. However, it is unknown which approaches are used in the other platforms to isolate containers for security reasons.

\section{Summary}

Cloud computing services, especially FEC, offer significantly improved applications to users and unlock a new revenue stream for MNOs. However, the security features of containerized services in a shared environment are not secure enough in the long term. The possible flaws and new types of attack vectors in these complex services threaten users, services, and data with exploits that endangering service confidentiality. Chapter \ref{section:isolation} describes ways to secure the systems to achieve an extra layer of isolation against internal and external attacks. The most promising solution, Kata Containers, combines the security of a VM and the resource efficiency of a container. The next chapter further analyzes Kata Containers.